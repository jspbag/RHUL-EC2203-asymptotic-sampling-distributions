---
pagetitle: Asymptotic sampling distributions
output: 
  revealjs::revealjs_presentation:
    incremental: false
    theme: solarized
    self_contained: false
    # reveal_plugins: ["menu","notes","chalkboard"]
    reveal_plugins: ["menu"]
    highlight: pygments
    center: true
    transition: none
    background_transition: none 
    reveal_options:
      # chalkboard:
      #   theme: whiteboard
      #   toggleNotesButton: true
      #   toggleChalkboardButton: true
      menu:
        numbers: true
      slideNumber: true
      previewLinks: false
    fig_caption: true
    css: mystyle.css
    
--- 

<section>

<h1>Asymptotic sampling distributions</h1>

Based on Stock and Watson, ch. 3

<br>

<h2>[Jesper Bagger](mailto:jesper.bagger@rhul.ac.uk)</h2>

<h3>EC2203 | Royal Holloway | 2020/21</h3>

</section>

# Asymptotic sampling distributions

## Large-sample approximations

- Random sampling induces a sampling distribution of any statistic computed on the sample, for example $\overline{Y}$

- Almost always unable to compute exact sampling distribution of $\overline{Y}$ or any other statistic of interest.

- Resort to **large-sample approximations**: compute the **asymptotic distribution** as $n$ grows large

- Key tools for large-sample approximations:

  - The Law of Large Numbers
  
  - The Central Limit Theorem
  
## Notation

- Suppose we are interested in a random variable $Y$ with population distribution $f_Y(y)$. Suppose that 

  $$\mathrm{E}(Y)=\mu_Y; \quad \mathrm{var}(Y) = \sigma_Y^2$$

- We have a size-$n$ i.i.d. sample of $Y$: $\{Y_i;i=1,...,n\}$

- We want to understand the large-sample ($n \rightarrow \infty$) properties of the sample average

  $$\overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i$$
  
## The Law of Large Numbers

- The **Law of Large Numbers (LLN)** states conditions under which $\overline{Y}$ is very close to $\mu_Y$ when $n$ is large.

- "Very close to" means that the probability that $\overline{Y}$ takes a value that is very close to $\mu_Y$ is very close to 1

- We use the shorthand $\overline{Y} \overset{p}{\rightarrow} \mu_Y$ and say that $\overline{Y}$ **converges in probability** to $\mu_Y$.

- LLN justifies replacing a population mean, like $\mu_Y$, by the corresponding sample average, $\overline{Y}$ in the statistical analysis

## The Law of Large Numbers

<div class="box">

Let $\{Y_i; i=1,...,n\}$ be i.i.d. random variables with $\mathrm{E}(Y_i) = \mu_Y$ and $\mathrm{var}(Y_i) = \sigma^2_Y$. If $\sigma^2_Y < \infty$, then

$$P(\mu_Y - \epsilon < \overline{Y} < \mu_Y + \epsilon) \rightarrow 1,$$

for any $\epsilon > 0$, as $n \rightarrow \infty$. Write $\overline{Y} \overset{p}{\rightarrow} \mu_Y$.

</div>

## The Central Limit Theorem

- The **Central Limit Theorem (CLT)** states conditions such that the distribution of $(\overline{Y} - \mu_Y)/\sigma_\overline{Y}$ is very close to the standard normal distribution when $n$ is large

- The CLT justifies the use of 

  $$\overline{Y} \sim \mathcal{N}(\mu_Y,\sigma_\overline{Y}^2); \quad \sigma_\overline{Y}^2 = \frac{1}{n}\sigma_Y^2$$

  to approximate the exact sampling distribution of $\overline{Y}$

<!-- - Remarkably, the approximation does not depend on the distribution of the individual $Y_i$'s in $\overline{Y}$. -->

## The Central Limit Theorem

<div class="box">

Let $\{Y_i; i=1,...,n\}$ be i.i.d. random variables with $\mathrm{E}(Y_i) = \mu_Y$ and $\mathrm{var}(Y_i) = \sigma^2_Y$. If $\sigma^2_Y < \infty$, then

$$\sqrt{n}(\overline{Y} - \mu_Y) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma_Y^2).$$

We say that $\sqrt{n}(\overline{Y} - \mu_Y)$ **converges in distribution** to the normal distribution $\mathcal{N}(0,\sigma_Y^2)$

</div>

# Summary

## Summary

- Finite-$n$ sampling distributions are complicated, but large-$n$ approximations based on LLN and CLT usually works well 

- The Law of Large Numbers gives conditions such that $\overline{Y}$ is very close to $\mu_Y$ in large samples 

  Justifies replacing an unknown population mean $\mu_Y$ by the known sample average $\overline{Y}$ in the statistical analysis

- The Central Limit Theorem gives conditions so distribution of $(\overline{Y} - \mu_Y)/\sigma_\overline{Y}$ is close to $\mathcal{N}(0,1)$ in large samples

  Justifies using Normal distribution for statistical inference
